@article{hossain2023,
  title={Autonomous Driving with Deep Reinforcement Learning in CARLA Simulation},
  author={Hossain, Jumman},
  journal={arXiv preprint arXiv:2306.11217},
  year={2023},
  url={https://arxiv.org/abs/2306.11217}
}

@article{zhu2020,
  title={Safe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving},
  author={Zhu, Meixin and Wang, Yinhai and Pu, Ziyuan and Hu, Jingyun and Wang, Xuesong and Ke, Ruimin},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={117},
  pages={102662},
  year={2020},
  publisher={Elsevier},
  url={https://meixinzhu.github.io/pdf/Safe_efficien_Zhu.pdf}
}

@phdthesis{zhu2021,
  title={Reinforcement Learning in Eco-driving for Connected and Automated Vehicles},
  author={Zhu, Zhaoxuan},
  school={Ohio State University},
  year={2021},
  url={https://rave.ohiolink.edu/etdc/view?acc_num=osu1638536531293253}
}

@article{survey2023,
  title={A survey of reinforcement learning theories in autonomous vehicles},
  author={Bai, S.},
  journal={Applied and Computational Engineering},
  volume={85},
  pages={187--194},
  year={2024},
  url={https://www.researchgate.net/publication/382758563_A_survey_of_reinforcement_learning_theories_in_autonomous_vehicles}
}

@article{jin2025,
  title={Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving},
  author={Jin, Guizhe and Li, Zhuoren and Leng, Bo and Han, Wei and Xiong, Lu and Sun, Chen},
  journal={arXiv preprint},
  volume={arXiv:2501.08096v1},
  year={2025},
  month={Jan},
  url={https://arxiv.org/abs/2501.08096}
}

@misc{metadrive,
      title={MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning}, 
      author={Quanyi Li and Zhenghao Peng and Lan Feng and Qihang Zhang and Zhenghai Xue and Bolei Zhou},
      year={2022},
      eprint={2109.12674},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.12674}, 
}

@article{selvaraj2023,
  AUTHOR = {Selvaraj, Dinesh Cyril and Hegde, Shailesh and Amati, Nicola and Deflorio, Francesco and Chiasserini, Carla Fabiana},
  TITLE = {A Deep Reinforcement Learning Approach for Efficient, Safe and Comfortable Driving},
  JOURNAL = {Applied Sciences},
  VOLUME = {13},
  YEAR = {2023},
  NUMBER = {9},
  ARTICLE-NUMBER = {5272},
  URL = {https://www.mdpi.com/2076-3417/13/9/5272},
  ISSN = {2076-3417},
  ABSTRACT = {Sensing, computing, and communication advancements allow vehicles to generate and collect massive amounts of data on their state and surroundings. Such richness of information fosters data-driven decision-making model development that considers the vehicle's environmental context. We propose a data-centric application of Adaptive Cruise Control employing Deep Reinforcement Learning (DRL). Our DRL approach considers multiple objectives, including safety, passengers' comfort, and efficient road capacity usage. We compare the proposed framework's performance to traditional ACC approaches by incorporating such schemes into the CoMoVe framework, which realistically models communication, traffic, and vehicle dynamics. Our solution offers excellent performance concerning stability, comfort, and efficient traffic flow in diverse real-world driving conditions. Notably, our DRL scheme can meet the desired values of road usage efficiency most of the time during the lead vehicle's speed-variation phases, with less than 40% surpassing the desirable headway. In contrast, its alternatives increase headway during such transient phases, exceeding the desired range 85% of the time, thus degrading performance by over 300% and potentially contributing to traffic instability. Furthermore, our results emphasize the importance of vehicle connectivity in collecting more data to enhance the ACC's performance.},
  DOI = {10.3390/app13095272}
}

@article{kiran2022,
  title={Deep Reinforcement Learning for Autonomous Driving: A Survey},
  author={Kiran, B. R. and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A. and Yogamani, Senthil and P{\'e}rez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={6},
  pages={4909-4926},
  year={2022},
  month={June},
  doi={10.1109/TITS.2021.3054625}
}

@article{lillicrap2016,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint},
  volume={arXiv:1612.04340},
  year={2016},
  eprint={1509.02971},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/pdf/1509.02971}
}

@inproceedings{li2019,
  title={Urban driving with multi-objective deep reinforcement learning},
  author={Li, Changliu and Czarnecki, Krzysztof},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={359--367},
  year={2019}
}

@inproceedings{yang2019,
  author = {Yang, Runzhe and Sun, Xingyuan and Narasimhan, Karthik},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4a46fbfca3f1465a27b210f4bdfe6ab3-Paper.pdf},
  volume = {32},
  year = {2019}
}

@article{zhang2023,
  title={Lexicographic actor-critic deep RL for urban autonomous driving},
  author={Zhang, Hengrui and Lin, Youfang and Han, Sheng and Lv, Kai},
  journal={IEEE Transactions on Vehicular Technology},
  volume={72},
  number={4},
  pages={4308--4319},
  year={2023},
  publisher={IEEE},
  keywords={Autonomous vehicles;Reinforcement learning;Task analysis;Optimization;Safety;Deep learning;Roads;Reinforcement learning;Urban autonomous driving;Actor-critic;Multi-objective},
  doi={10.1109/TVT.2022.3226579}
}

@ARTICLE{xu2018,
  author={Xu, Xin and Zuo, Lei and Li, Xin and Qian, Lilin and Ren, Junkai and Sun, Zhenping},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways}, 
  year={2020},
  volume={50},
  number={10},
  pages={3884-3897},
  keywords={Decision making;Intelligent vehicles;Road transportation;Approximation algorithms;Vehicle dynamics;Function approximation;Uncertainty;Autonomous decision-making;intelligent driving vehicles;Markov decision processes (MDPs);multiobjective;reinforcement learning (RL);value function approximation},
  doi={10.1109/TSMC.2018.2870983}
}

@INPROCEEDINGS{kendall2019,
  author={Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={Learning to Drive in a Day}, 
  year={2019},
  volume={},
  number={},
  pages={8248-8254},
  keywords={Reinforcement learning;Autonomous vehicles;Task analysis;Markov processes;Global Positioning System;Sensors;Training},
  doi={10.1109/ICRA.2019.8793742}
}

@InProceedings{pmlr-v78-dosovitskiy17a,
  title = 	 {{CARLA}: {An} Open Urban Driving Simulator},
  author = 	 {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = 	 {1--16},
  year = 	 {2017},
  editor = 	 {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  volume = 	 {78},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf},
  url = 	 {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
  abstract = 	 {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research.}
}

@INPROCEEDINGS{lopez2018,
  author={Lopez, Pablo Alvarez and Behrisch, Michael and Bieker-Walz, Laura and Erdmann, Jakob and Flötteröd, Yun-Pang and Hilbrich, Robert and Lücken, Leonhard and Rummel, Johannes and Wagner, Peter and Wiessner, Evamarie},
  booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Microscopic Traffic Simulation using SUMO}, 
  year={2018},
  volume={},
  number={},
  pages={2575-2582},
  keywords={Tools;Microscopy;Roads;Data models;Urban areas;Vehicle dynamics;Mathematical model},
  doi={10.1109/ITSC.2018.8569938}
}

@inproceedings{sallab2016,
  title={End-to-end deep reinforcement learning for lane keeping assist},
  author={Al-Sallab, Ahmad EL and Abdou, Mahmoud and Perot, Etienne and Yogamani, Senthil},
  booktitle={NIPS 2016 Workshop on Machine Learning for Intelligent Transportation Systems},
  year={2016}
}

@article{he2023,
  title={Towards Energy-Efficient Autonomous Driving: A Multi-Objective Reinforcement Learning Approach},
  author={He, Xiangkun and Lv, Chen},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={10},
  number={JAS-2022-1495},
  pages={1329},
  year={2023},
  issn={2329-9266},
  doi={10.1109/JAS.2023.123378},
  url={https://www.ieee-jas.net/en/article/doi/10.1109/JAS.2023.123378}
}